{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"STAT 992: Science of Large Language Models","text":"<p>Term: Spring 2026 Meeting time: Tue/Thu 4:00\u20135:15pm, from Jan 20 to Mar 8    Location: Morgridge Hall 2538, 1205 University Avenue, Madison Instructor: Yiqiao Zhong Email: MyFirstName dot MyLastName at wisc doc edu Q&amp;A: Canvas Discussion Page</p>"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>Schedule</li> <li>Resources</li> </ul>"},{"location":"#announcements","title":"Announcements","text":"<p>Feb 11, 2026</p> <p>I will be giving a talk \u201cDo LLMs reason as we do? A synthetic study of transformers\u2019 learning dynamics for compositions\u201d at Machine Learning Lunch and Meetings (MLLM) on Feb 17, 2026, starting from 12:15 pm at\u00a0Morgridge Hall 7560.  </p> <p>Jan 29, 2026</p> <p>One exciting local event AI Meets Society (AIMS) Symposium is scheduled on February 21st, 2026. Many great faculty and researchers will share perspectives on the future of AI. Consider registering NOW!</p> <p>Jan 27, 2026</p> <p>We are slightly changing the class format: at each meeting, we will now begin with a discussion of the previous lecture, followed by the new lecture.</p> <p>Jan 20, 2026</p> <p>Welcome! Please check out the schedule and add your info to the Google doc.</p>"},{"location":"#course-description","title":"Course description","text":"<p>This is a new topic course focusing on interpretability and understanding the internal mechanisms of large language models (LLMs). Since GPT-3, LLMs are rapidly advancing in their capabilities, yet we don\u2019t have a good understanding of how they operate, and why they work or not work. We will cover the fundamentals of LLMs, new phenemena, mathematical structures, statistical techinques, and their applications in sciences.</p> <p>The goal is to bring interactions across different departments (Stats, CS, ECE, BMI, math, etc.). This is a one-credit, seven-week long course with minimal workload. We will meet twice each week, and each meeting will involve a mix of lectures and discussions.</p> <p>A tentative outline of the course is the following; see Schedule for details.</p> <ul> <li>Week 1\u20132: Emergent phenomena in LLMs: basics of transformers, emergent abilities and grokking, prompting and in-context learning, out-of-distribution generalization, induction heads, chain-of-thought reasoning.</li> <li>Week 3\u20134: Mathematical structures of LLMs: linear representation hypothesis, feature superposition, sparsity and low-rankness in embeddings, layerwise analysis, near orthogonal representation.</li> <li>Week 5\u20136: Statistical techniques for LLMs: PCA and factor analysis, dictionary learning (SAE), causal tracing and circuits, leave-one-out, influence functions.</li> <li>Week 7: Case studies in domain applications: genomics foundation models, watermarking, memorization and copyrights. </li> </ul>"},{"location":"#logistics","title":"Logistics","text":"<ul> <li>Class format: Each meeting consists of two parts: (i) a short lecture (mostly by myself) that introduces the bascis of a topic in LLMs, (ii) 20\u201330 mins in-class discussions led by two participants. </li> <li>In-class discussions: The participants need to prepare a few slides to initiate the discussions. The participants are encouraged to use their own research background and knowledge to provide complementary perspectives or critiques of the lectures; they are also welcome to raise concerns about LLMs or share preliminary research ideas.</li> <li>Grading: No homework, exam, or course projects. Grading is based on class participation and in-class discussions.</li> <li>Additional questions: Please use the Canvas page to ask addtional questions outside of classes.</li> </ul>"},{"location":"#note","title":"Note","text":"<p>This webpage is built with MkDocs following the template.</p>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#basics-of-deep-learning-and-llms","title":"Basics of deep learning and LLMs","text":"<ul> <li>Introduction to Deep Learning by Sebastian Raschka</li> <li>NanoGPT by Andrey Karpathy - simplest transformer built from scratch</li> <li>Vardan Papyan\u2019s course on analysis of deep learning</li> </ul>"},{"location":"resources/#topics-on-interpretability-and-ai-safety","title":"Topics on interpretability and AI safety","text":"<ul> <li>Overview of mechanistic interpretability</li> <li>Coefficient giving (formerly Open Philanthropy) - A call for AI safety research</li> <li>My research blogs </li> <li>Wisconsin AI Safety Initiative</li> </ul>"},{"location":"resources/#interviews-and-perspectives","title":"Interviews and perspectives","text":"<ul> <li>Ilya Sutskever \u2013 We\u2019re moving from the age of scaling to the age of research</li> <li>Andrej Karpathy - AGI is still a decade away</li> <li>Richard Sutton \u2013 Father of RL thinks LLMs are a dead end</li> </ul>"},{"location":"schedule/","title":"Schedule","text":"<p>This is the schedule for all the meetings. You can find the topics, the slides, and readings here.</p> <p>Here is the general outline.</p> <ul> <li>Week 1\u20132: Emergent phenomena in LLMs</li> <li>Week 3\u20134: Mathematical structures of LLMs</li> <li>Week 5\u20136: Statistical techniques for LLMs</li> <li>Week 7: Case studies in domain applications</li> </ul> <p>Legend: - \ud83d\udcc4 Reading, \ud83d\udda5\ufe0f Slides, \ud83e\uddea Notebook/code, \ud83d\udc65 Discussion initiators</p>"},{"location":"schedule/#weeks","title":"Weeks","text":"Week Date Topic Materials Discussants 1 Jan 20 Intro + transformers basics \ud83d\udda5\ufe0f L01 Matthias Katzfu\u00df and Maja Waldro 1 Jan 22 Emergent abilities, prompting and in-context learning \ud83d\udda5\ufe0f L02 Jack Sperling and Brendan Joyce 2 Jan 27 Out-of-distribution generalization, induction heads \ud83d\udda5\ufe0f L03 Samuel Yeh and Eva Song 2 Jan 29 Chain-of-thought reasoning, reinforecement learning \ud83d\udda5\ufe0f L04 Paul Kantor and Zhiqi Gao 3 Feb 3 Linear representation hypothesis, feature superposition \ud83d\udda5\ufe0f L05 Ishita Kakkar and Sam Baumohl 3 Feb 5 Sparsity and low-rankness \ud83d\udda5\ufe0f L06 Yupeng Zhang and Peter Zhao 4 Feb 10 Layerwise structures of embeddings \ud83d\udda5\ufe0f L07 Bofeng Cao and Shixiao Liang 4 Feb 12 Reasoning trace and self-reflection \ud83d\udda5\ufe0f L08 \u2026 5 Feb 17 PCA and factor analysis (steering, model editing, interpretability) \u2026 \u2026 5 Feb 19 Dictionary learning, SAE (feature interpretability) \u2026 \u2026 6 Feb 24 Causal tracing and circuits (attribution, interpretability) \u2026 \u2026 6 Feb 26 Leave-one-out, influence functions (robustness, sensitivity analysis) \u2026 \u2026 7 Mar 3 Genomics foundation models \u2026 \u2026 7 Mar 5 Watermarking, memorization \u2026 \u2026"},{"location":"schedule/#notes","title":"Notes","text":"<ul> <li>Schedule may shift; the table above will be frequently updated</li> <li>Some illustrative figures and tables in the slides are AI-generated; please use with caution.</li> </ul>"},{"location":"tmp/course_schedule/","title":"Course schedule","text":"start end topic 10:00 10:30 coffee! 12:00 13:00 lunch! <p>Generate markdown tables at tablesgenerator.com</p>"},{"location":"tmp/precourse/","title":"Precourse","text":"<p>Any precourse preparations go here. </p>"}]}