{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"STAT 992: Science of Large Language Models","text":"<p>Term: Spring 2026 Meeting time: Tue/Thu 4:00\u20135:15pm, from Jan 20 to Mar 8    Location: Morgridge Hall 2538, 1205 University Avenue, Madison Instructor: Yiqiao Zhong Email: MyFirstName dot MyLastName at wisc doc edu Q&amp;A: Canvas Discussion Page</p>"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>Schedule</li> <li>Resources</li> </ul>"},{"location":"#announcements","title":"Announcements","text":"<p>Jan 29, 2026</p> <p>One exciting local event AI Meets Society (AIMS) Symposium is scheduled on February 21st, 2026. Many great faculty and researchers will share perspectives on the future of AI. Consider registering NOW!</p> <p>Jan 27, 2026</p> <p>We are slightly changing the class format: at each meeting, we will now begin with a discussion of the previous lecture, followed by the new lecture.</p> <p>Jan 20, 2026</p> <p>Welcome! Please check out the schedule and add your info to the Google doc.</p>"},{"location":"#course-description","title":"Course description","text":"<p>This is a new topic course focusing on interpretability and understanding the internal mechanisms of large language models (LLMs). Since GPT-3, LLMs are rapidly advancing in their capabilities, yet we don\u2019t have a good understanding of how they operate, and why they work or not work. We will cover the fundamentals of LLMs, new phenemena, mathematical structures, statistical techinques, and their applications in sciences.</p> <p>The goal is to bring interactions across different departments (Stats, CS, ECE, BMI, math, etc.). This is a one-credit, seven-week long course with minimal workload. We will meet twice each week, and each meeting will involve a mix of lectures and discussions.</p> <p>A tentative outline of the course is the following; see Schedule for details.</p> <ul> <li>Week 1\u20132: Emergent phenomena in LLMs: basics of transformers, emergent abilities and grokking, prompting and in-context learning, out-of-distribution generalization, induction heads, chain-of-thought reasoning.</li> <li>Week 3\u20134: Mathematical structures of LLMs: linear representation hypothesis, feature superposition, sparsity and low-rankness in embeddings, layerwise analysis, near orthogonal representation.</li> <li>Week 5\u20136: Statistical techniques for LLMs: PCA and factor analysis, dictionary learning (SAE), causal tracing and circuits, leave-one-out, influence functions.</li> <li>Week 7: Case studies in domain applications: genomics foundation models, watermarking, memorization and copyrights. </li> </ul>"},{"location":"#logistics","title":"Logistics","text":"<ul> <li>Class format: Each meeting consists of two parts: (i) a short lecture (mostly by myself) that introduces the bascis of a topic in LLMs, (ii) 20\u201330 mins in-class discussions led by two participants. </li> <li>In-class discussions: The participants need to prepare a few slides to initiate the discussions. The participants are encouraged to use their own research background and knowledge to provide complementary perspectives or critiques of the lectures; they are also welcome to raise concerns about LLMs or share preliminary research ideas.</li> <li>Grading: No homework, exam, or course projects. Grading is based on class participation and in-class discussions.</li> <li>Additional questions: Please use the Canvas page to ask addtional questions outside of classes.</li> </ul>"},{"location":"#note","title":"Note","text":"<p>This webpage is built with MkDocs following the template.</p>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#setup","title":"Setup","text":"<ul> <li>Python (recommended version)</li> <li>Conda / mamba</li> <li>PyTorch install instructions</li> <li>(Optional) CUDA notes</li> </ul>"},{"location":"resources/#reference","title":"Reference","text":"<ul> <li>Math review: \u2026</li> <li>Linear algebra/probability refreshers: \u2026</li> <li>Tips for debugging training runs</li> </ul>"},{"location":"resources/#course-tools","title":"Course tools","text":"<ul> <li>Canvas: \u2026</li> <li>Piazza/Ed: \u2026</li> <li>GitHub repo (if any): \u2026</li> </ul>"},{"location":"schedule/","title":"Schedule","text":"<p>This is the schedule for all the meetings. You can find the topics, the slides, and readings here.</p> <p>Here is the general outline.</p> <ul> <li>Week 1\u20132: Emergent phenomena in LLMs</li> <li>Week 3\u20134: Mathematical structures of LLMs</li> <li>Week 5\u20136: Statistical techniques for LLMs</li> <li>Week 7: Case studies in domain applications</li> </ul> <p>Legend: - \ud83d\udcc4 Reading, \ud83d\udda5\ufe0f Slides, \ud83e\uddea Notebook/code, \ud83d\udc65 Discussion initiators</p>"},{"location":"schedule/#weeks","title":"Weeks","text":"Week Date Topic Materials Discussants 1 Jan 20 Intro + transformers basics \ud83d\udda5\ufe0f L01 Matthias Katzfu\u00df and Maja Waldro 1 Jan 22 Emergent abilities, prompting and in-context learning \ud83d\udda5\ufe0f L02 Jack Sperling and Brendan Joyce 2 Jan 27 Out-of-distribution generalization, induction heads \ud83d\udda5\ufe0f L03 \u2026 2 Jan 29 Chain-of-thought reasoning, reinforecement learning \u2026 \u2026 3 Feb 3 Linear representation hypothesis, feature superposition \u2026 \u2026 3 Feb 5 Sparsity and low-rankness \u2026 \u2026 4 Feb 10 Layerwise structures of embeddings \u2026 \u2026 4 Feb 12 Reasoning trace and self-reflection \u2026 \u2026 5 Feb 17 PCA and factor analysis (steering, model editing, interpretability) \u2026 \u2026 5 Feb 19 Dictionary learning, SAE (feature interpretability) \u2026 \u2026 6 Feb 24 Causal tracing and circuits (attribution, interpretability) \u2026 \u2026 6 Feb 26 Leave-one-out, influence functions (robustness, sensitivity analysis) \u2026 \u2026 7 Mar 3 Genomics foundation models \u2026 \u2026 7 Mar 5 Watermarking, memorization \u2026 \u2026"},{"location":"schedule/#notes","title":"Notes","text":"<ul> <li>Schedule may shift; the table above will be frequently updated</li> <li>Some illustrative figures and tables in the slides are AI-generated; please use with caution.</li> </ul>"},{"location":"tmp/course_schedule/","title":"Course schedule","text":"start end topic 10:00 10:30 coffee! 12:00 13:00 lunch! <p>Generate markdown tables at tablesgenerator.com</p>"},{"location":"tmp/precourse/","title":"Precourse","text":"<p>Any precourse preparations go here. </p>"}]}